<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>文献阅读3|EfficientNet | 方方土'Blog</title><meta name="keywords" content="计算机视觉,EfficientNet"><meta name="author" content="方方土同学"><meta name="copyright" content="方方土同学"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="文献阅读3|EfficientNet: Rethinking Model Scaling for Convolutional Neural NetworksIntrudoction在论文中，作者介绍了放大卷积神经网络是一种常见的提高模型准确率的方法。但是在传统的方法中，通常只是在某单一维度上进行放大（宽度width，深度depth，图片分辨率resolution），宽度就是网络中的过滤器的数量，因">
<meta property="og:type" content="article">
<meta property="og:title" content="文献阅读3|EfficientNet">
<meta property="og:url" content="http://pengjoy1106.top/2022/04/04/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB3/index.html">
<meta property="og:site_name" content="方方土&#39;Blog">
<meta property="og:description" content="文献阅读3|EfficientNet: Rethinking Model Scaling for Convolutional Neural NetworksIntrudoction在论文中，作者介绍了放大卷积神经网络是一种常见的提高模型准确率的方法。但是在传统的方法中，通常只是在某单一维度上进行放大（宽度width，深度depth，图片分辨率resolution），宽度就是网络中的过滤器的数量，因">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220410110849.png">
<meta property="article:published_time" content="2022-04-04T14:00:00.000Z">
<meta property="article:modified_time" content="2022-04-10T02:58:05.709Z">
<meta property="article:author" content="方方土同学">
<meta property="article:tag" content="计算机视觉">
<meta property="article:tag" content="EfficientNet">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220410110849.png"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/kakarotto (8).jpg"><link rel="canonical" href="http://pengjoy1106.top/2022/04/04/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB3/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '文献阅读3|EfficientNet',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-04-10 10:58:05'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/custom.css"><script src="/live2d-widget-master/autoload.js"></script><script src="/live2d-widget/autoload.js"></script><meta name="generator" content="Hexo 6.1.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/kakarotto (8).jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/"><i class="fa-fw fas fa-file-video"></i><span> 光影旅途</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220410110849.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">方方土'Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/"><i class="fa-fw fas fa-file-video"></i><span> 光影旅途</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">文献阅读3|EfficientNet</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-04T14:00:00.000Z" title="发表于 2022-04-04 22:00:00">2022-04-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-04-10T02:58:05.709Z" title="更新于 2022-04-10 10:58:05">2022-04-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="文献阅读3|EfficientNet"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="文献阅读3-EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks"><a href="#文献阅读3-EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks" class="headerlink" title="文献阅读3|EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"></a>文献阅读3|EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</h1><h2 id="Intrudoction"><a href="#Intrudoction" class="headerlink" title="Intrudoction"></a>Intrudoction</h2><p>在论文中，作者介绍了<strong>放大卷积神经网络</strong>是一种常见的提高模型准确率的方法。但是在传统的方法中，通常只是在某单一维度上进行放大（<strong>宽度width</strong>，<strong>深度depth</strong>，<strong>图片分辨率resolution</strong>），宽度就是网络中的过滤器的数量，因为增加了过滤器的数量，该层的输出的通道数就相应变大了，深度可以理解为整个网络结构的长度，即网络中layer的数量。那么为什么在这几个维度上进行放大可以提高准确率？因为增加了图片的分辨率或则增加了网络的宽度，网络就能够捕获到更过细节的特征，而增加网络的深度能够捕获到更丰富和更复杂的特征。<br>虽然也可以任意的放大两个或三个维度，但是因为维度变多，设计空间也随之变大，因此随意的放大多个维度需要耗费较大的人力来调整，并且也通常会一个次优的精度和效率。因此作者通过研究实验提出了一种新的缩放方法——复合缩放方法(compound scaling method)。</p>
<p>在一些手工设计网络中(如AlexNET、VGG、ResNet等)，我们常常会有这样的疑问：为什么输入图像分辨率要固定为224，为什么卷积的个数要设置为这个值？为什么网络的深度设为这么深？这些问题你要问设计作者的话，估计回复就四个字——工程经验。<br>这篇论文使用NAS(Neural Architecture Search)技术来搜索网络的图像输入分辨率r，网络的深度depth，以及channel的宽度width三个参数的合理化配置。<br>        EfficientNetB0到B7与其他网络的对比如下图所示:</p>
<p><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402161317.png"></p>
<p>为什么在这几个维度上进行放大可以提高准确率呢？因为增加了图片的分辨率或则增加了网络的宽度，网络就能够捕获到更过细节的特征，而增加网络的深度能够捕获到更丰富和更复杂的特征。</p>
<p>虽然也可以任意的放大两个或三个维度，但是因为维度变多，设计空间也随之变大，因此随意的放大多个维度需要耗费较大的人力来调整，并且也通常会有一个次优的精度和效率。因此作者通过研究实验提出了一种新的缩放方法——**复合缩放方法(compound scaling method)**。<br>下图所展示的便是放大神经网络的几种方法：</p>
<p><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402162025.png"></p>
<p>在之前的论文中，有的会通过增加网络的<strong>width</strong>即增加卷积核的个数(增加特征矩阵的<strong>channels</strong>)来提升网络的性能(上图b)，有的会通过增加网络的<strong>深度</strong>即<strong>使用更多的层结构</strong>来提升网络的性能(上图c)，有的会通过增加输入网络的分辨率来提升网络的性能(上图d)。而在本篇论文中会同时增加网络的<strong>width</strong>、网络的<strong>深度</strong>以及输入网络的<strong>分辨率</strong>来提升网络的性能(上图e)</p>
<p>但是因为网络结构的缩放并不会改变具体某一层的卷积操作，所以一个良好的基线网络是必须的，作者在论文中也提出了一种新的基线网络结构——<strong>EfficientNet</strong>。</p>
<h2 id="compound-scaling-method"><a href="#compound-scaling-method" class="headerlink" title="compound scaling method"></a>compound scaling method</h2><h3 id="论文思想"><a href="#论文思想" class="headerlink" title="论文思想"></a>论文思想</h3><ul>
<li><p><strong>Depth（d）</strong>：缩放网络深度是许多卷积网络中最常用的方法。<em>更深的卷积网络能够捕获到更丰富和复杂的特征，但是更深的网络由于存在<strong>梯度消失</strong>的问题而难以训练。</em>尽管有一些方法可以解决梯度消失（例如 跨层连接skip connections和批量归一化 batch normalization），但是对于非常深的网络所获得的准确率的增益会减弱。例如ResNet-1000和ResNet-101有着相近的准确率尽管depth相差很大。<em>下图的中间的曲线图表示用不同的深度系数d缩放模型的准确率曲线，并且表明了对于非常深的网络，准确率的增益会减弱。</em></p>
<blockquote>
<p>The intuition is that deeper ConvNet can capture richer and more complex features, and generalize well on new tasks. However, deeper networks are also more difficult to train due to the vanishing gradient problem</p>
</blockquote>
</li>
<li><p><strong>Width（w）</strong>：缩放网络宽度对于小规模的网络也是很常用的一种方式。<em>更宽的网络更能够捕捉到更多细节的特征，也更容易训练。</em>很宽但很浅的网络结构很难捕捉到更高层次的特征。<em>下图中左边的曲线图则是作者的不同宽度系数实验结果曲线，当w不断增大的时候，准确率很快就饱和了。</em></p>
<blockquote>
<p>wider networks tend to be able to capture more fine-grained features and are easier to train. However, extremely wide but shallow networks tend to have difficulties in capturing higher level features.</p>
</blockquote>
</li>
<li><p><strong>Resolution（r）</strong>：使用更高分辨率的图像，网络能够捕获到更细粒度的特模式。增加输入网络的图像分辨率能够潜在得获得更高细粒度的特征模板，但对于非常高的输入分辨率，<em>准确率的增益也会减小，并且大分辨率图像会增加计算量</em>。<em>下图中右边的曲线图则是作者的不同分辨率系数实验结果曲线，对于非常高分辨率的图像，准确率的增益会减弱。（r&#x3D;1.0表示224x224，r&#x3D;2.5表示560x560）。</em></p>
<blockquote>
<p>With higher resolution input images, ConvNets can potentially capture more fine-grained patterns. but the accuracy gain diminishes for very high resolutions.</p>
</blockquote>
</li>
</ul>
<p>下图展示了在基准<strong>EfficientNetB-0</strong>上分别增加<strong>width</strong>、<strong>depth</strong>以及<strong>resolution</strong>后得到的统计结果。通过下图可以看出大概在Accuracy达到80%时就趋于饱和了。</p>
<p><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402203414.png"></p>
<p>通过以上实验得出<strong>结论1：对网络深度、宽度和分辨率中的任一维度进行缩放都可以提高精度，但是当模型非常大时，这种放大的增益都会减弱</strong>。</p>
<p>接着作者又做了一个实验，采用不同的d , r 组合，然后不断改变网络的width就得到了如下图所示的4条曲线，通过分析可以发现在相同的FLOPs下，同时增加d和r的效果最好。</p>
<h3 id="复合缩放Compound-Scaling"><a href="#复合缩放Compound-Scaling" class="headerlink" title="复合缩放Compound Scaling"></a>复合缩放<strong>Compound Scaling</strong></h3><blockquote>
<p>作者通过实验发现缩放的各个维度并不是独立的。直观上来讲，对于分辨率更高的图像，我们应该增加网络深度，因为需要更大的感受野来帮助捕获更多像素点的类似特征。为了证明这种猜测，作者做了一下相关实验：比较宽度缩放在不同深度和分辨率之下对准确率的影响。</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402204530.png"></p>
<p><em>通过上图的结果我们可以看到d&#x3D;2.0，r&#x3D;1.3时宽度缩放在相同flops下有着更高的准确率。</em><br>得到<strong>结论2：为了达到更好的准确率和效率，在缩放时平衡网络所有维度至关重要。</strong></p>
<p>为了方便后续理解，我们先看下论文中通过<strong>NAS</strong>(<strong>Neural Architecture Search</strong>)技术搜索得到的EfficientNetB0的结构，如下图所示，整个网络框架由一系列Stage，$\widehat{Fi}$表示对应stage的运算操作，$\widehat{Li}$表示在该stage中重复$\widehat{Fi}$的次数：</p>
<p><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402210952.png"></p>
<p>作者在论文中对整个网络的运算进行抽象：$N(d,\omega,r) &#x3D; ^{\bigodot}<em>{i&#x3D;1…s}F_i^{L_i}(X</em>{<H_iW_iC_i>})$</p>
<p>其中：</p>
<ul>
<li>$ ^{\bigodot}_{i&#x3D;1…s}$表示连乘运算</li>
<li>$F_i$表示一个运算操作(如上图中的<strong>operator</strong>)，那么$F_i^{L_i}$表示在Stage_i中$F_i$运算被重复执行$L_i$次。</li>
<li>X表示输入Stage_i的特征矩阵(<strong>input tensor</strong>)</li>
<li>$&lt;H_i,W_i,C_i&gt;$表示X的高度，宽度以及<strong>Channels</strong>(<strong>shape</strong>)。</li>
</ul>
<p>为了探究d,r,w这三个因子对最终准确率的影响，作者将d,r,w加到公式中，我们可以得到抽象化后的优化问题(在指定资源限制下)，其中s.t.代表限制条件：</p>
<blockquote>
<p>Our target is to maximize the model accuracy for any given resource constraints, which can be formulated as an optimization problem:</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220403140239.png"></p>
<p>其中：</p>
<ul>
<li>d用来缩放深度$\widehat{Li}$</li>
<li>r用来缩放分辨率即影响$\widehat{Hi}$和$\widehat{Wi}$</li>
<li>$\omega$用来缩放特征矩阵的channel即$\widehat{Ci}$</li>
<li>target_memory为memory限制</li>
<li>target_flops为FlOPs限制</li>
</ul>
<p>然后，作者又提出了一种新的复合缩放方法使用了一个复合系数<strong>ϕ</strong> ，通过这个系数按照以下原则来统一的缩放<strong>网络深度</strong>、<strong>宽度</strong>和<strong>分辨率</strong>：</p>
<p><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402204755.png"></p>
<p>这里：</p>
<ul>
<li>FLOPs(理论计算量)与depth的关系是：当depth翻倍，FLOPs也翻倍。</li>
<li>FLOPs与width的关系是：当width翻倍(即channel翻倍)，FLOPs会翻4倍，因为卷积层的FLOPs约等于$feature_\omega \times feature_h \times feature_c \times kernel_\omega \times kernel_h \times kernel_{number}$(假设输入输出特征矩阵的高宽不变)，当width翻倍，输入特征矩阵的channels($feature_c$)和输出特征矩阵的channel或卷积核的个数($kernel_{number}$)都会翻倍，所以FLOPs会翻4倍。</li>
<li>FLOPs与resolution的关系是：当resolution翻倍，FLOPs也会翻4倍，和上面类似因为特征矩阵的宽度$feature_\omega$和特征矩阵的高度$feature_h$都会翻倍。</li>
</ul>
<p>所以总的FLOPs倍率可以近似用($({\alpha \cdot \beta^2 \cdot \gamma^2})^\phi$)来表示，当限制${\alpha \cdot \beta^2 \cdot \gamma^2} \approx 2$，对于任意一个$\phi$而言FLOPs想当增加了$2^\phi$倍</p>
<p>接下来作者在基准网络EfficientNetB-0上使用NAS来搜索$\alpha,\beta,\gamma$这三个参数。</p>
<ol>
<li>首先固定$\phi &#x3D; 1$，并基于上面给出的公式(2)和(3)进行搜索，作者发现对于EfficientNetB-0最佳参数为</li>
<li>接着固定$\alpha&#x3D;1.2,\beta&#x3D;1.1,\gamma&#x3D;1.15$，在EfficientNetB-0的基础上使用不同的$\phi$分别得到EfficientNetB-1至EfficientNetB-7。</li>
</ol>
<p>需要注意的是，对于不同的基准网络搜索出的α , β , γ 也不一定相同。还需要注意的是，在原论文中，作者也说了，如果直接在大模型上去搜索α , β , γ 可能获得更好的结果，但是在较大的模型中搜索成本太大，所以这篇文章就在比较小的EfficientNetB-0模型上进行搜索的。</p>
<blockquote>
<p>Notably, it is possible to achieve even better performance by searching for α, β, γ directly around a large model, but the search cost becomes prohibitively more expensive on larger models. Our method solves this issue by only doing search once on the small baseline network (step 1), and then use the same scaling coefficients for all other models (step 2).</p>
</blockquote>
<h2 id="网络详细结构"><a href="#网络详细结构" class="headerlink" title="网络详细结构"></a>网络详细结构</h2><p>下表为EfficientNet-B0的网络框架（B1-B7就是在B0的基础上修改<strong>Resolution</strong>，<strong>Channels</strong>以及<strong>Layers</strong>），可以看出网络总共分成了9个<strong>Stage</strong>，第一个Stage就是一个卷积核大小为3x3步距为2的普通卷积层（包含BN和激活函数<strong>Swish</strong>），<strong>Stage2～Stage8</strong>都是在重复堆叠<strong>MBConv</strong>结构（最后一列的<strong>Layers</strong>表示该<strong>Stage</strong>重复<strong>MBConv</strong>结构多少次），而<strong>Stage9</strong>由一个普通的1x1的卷积层（包含BN和激活函数<strong>Swish</strong>）一个平均池化层和一个全连接层组成。表格中每个<strong>MBConv</strong>后会跟一个数字1或6，这里的1或6就是倍率因子n即<strong>MBConv</strong>中第一个1x1的卷积层会将输入特征矩阵的<strong>channels</strong>扩充为n倍，其中k3x3或k5x5表示<strong>MBConv</strong>中<strong>Depthwise Conv</strong>所采用的卷积核大小。<strong>Channels</strong>表示通过该<strong>Stage</strong>后输出特征矩阵的Channels。</p>
<p><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220403144813.png"></p>
<h3 id="MBConv"><a href="#MBConv" class="headerlink" title="MBConv"></a>MBConv</h3><p>MBConv其实就是MobileNetV3网络中的InvertedResidualBlock，但也有些许区别。一个是采用的激活函数不一样(EfficientNet的MBConv中使用的都是Swish激活函数)，另一个是在每个MBConv中都加入了SE(Squeeze-and-Excitation)模块。</p>
<p>以下结构图为B站UP主<a target="_blank" rel="noopener" href="https://space.bilibili.com/18161609">霹雳吧啦Wz</a>绘制的MBConv结构。</p>
<p><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220403145314.png"></p>
<p>如图所示，<strong>MBConv</strong>结构主要由一个1x1的普通卷积（升维作用，包含<strong>BN</strong>和<strong>Swish</strong>），一个<strong>kxk</strong>的<strong>Depthwise Conv</strong>卷积（包含<strong>BN</strong>和<strong>Swish</strong>）<strong>k</strong>的具体值可看<strong>EfficientNet-B0</strong>的网络框架主要有3x3和5x5两种情况，一个SE模块，一个1x1的普通卷积（降维作用，包含BN），一个<strong>Droupout</strong>层构成。搭建过程中还需要注意几点：</p>
<ul>
<li>第一个升维的<strong>1x1</strong>卷积层，它的卷积核个数是输入特征矩阵<strong>channel</strong>的n倍， $n \in \left{1, 6\right}$(n∈{1,6})。</li>
<li>当n &#x3D; 1时，不要第一个升维的1x1卷积层，即<strong>Stage2</strong>中的<strong>MBConv</strong>结构都没有第一个升维的1x1卷积层（这和<strong>MobileNetV3</strong>网络类似）。</li>
<li>关于<strong>shortcut</strong>连接，仅当输入<strong>MBConv</strong>结构的特征矩阵与输出的特征矩阵<strong>shape</strong>相同时才存在（代码中可通过$stride&#x3D;&#x3D;1 and inputc_channels&#x3D;&#x3D;output_channels$条件来判断）。</li>
<li>SE模块如下所示，由&#x3D;&#x3D;一个全局平均池化&#x3D;&#x3D;，&#x3D;&#x3D;两个全连接层&#x3D;&#x3D;组成。第一个全连接层的节点个数是输入该MBConv特征矩阵channels的$\frac{1}{4}$ 且使用Swish激活函数。第二个全连接层的节点个数等于Depthwise Conv层输出的特征矩阵channels，且使用Sigmoid激活函数。</li>
<li>Dropout层的dropout_rate在tensorflow的keras源码中对应的是drop_connect_rate后面会细讲（注意，在源码实现中只有使用shortcut的时候才有Dropout层）。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220403153958.png"></p>
<h3 id="EfficientNet-B0-B7-参数"><a href="#EfficientNet-B0-B7-参数" class="headerlink" title="EfficientNet(B0-B7)参数"></a>EfficientNet(B0-B7)参数</h3><table>
<thead>
<tr>
<th align="center">Model</th>
<th align="center">input_size</th>
<th align="center">width_coefficient</th>
<th align="center">depth_coefficient</th>
<th align="center">drop_connect_rate</th>
<th align="center">dropout_rate</th>
</tr>
</thead>
<tbody><tr>
<td align="center">EfficientNetB0</td>
<td align="center">224x224</td>
<td align="center">1.0</td>
<td align="center">1.0</td>
<td align="center">0.2</td>
<td align="center">0.2</td>
</tr>
<tr>
<td align="center">EfficientNetB1</td>
<td align="center">240×240</td>
<td align="center">1.0</td>
<td align="center">1.1</td>
<td align="center">0.2</td>
<td align="center">0.2</td>
</tr>
<tr>
<td align="center">EfficientNetB2</td>
<td align="center">260x260</td>
<td align="center">1.1</td>
<td align="center">1.2</td>
<td align="center">0.2</td>
<td align="center">0.3</td>
</tr>
<tr>
<td align="center">EfficientNetB3</td>
<td align="center">300x300</td>
<td align="center">1.2</td>
<td align="center">1.4</td>
<td align="center">0.2</td>
<td align="center">0.3</td>
</tr>
<tr>
<td align="center">EfficientNetB4</td>
<td align="center">380x380</td>
<td align="center">1.4</td>
<td align="center">1.8</td>
<td align="center">0.2</td>
<td align="center">0.4</td>
</tr>
<tr>
<td align="center">EfficientNetB5</td>
<td align="center">456x456</td>
<td align="center">1.6</td>
<td align="center">2.2</td>
<td align="center">0.2</td>
<td align="center">0.4</td>
</tr>
<tr>
<td align="center">EfficientNetB6</td>
<td align="center">528x528</td>
<td align="center">1.8</td>
<td align="center">2.6</td>
<td align="center">0.2</td>
<td align="center">0.5</td>
</tr>
<tr>
<td align="center">EfficientNetB7</td>
<td align="center">600x600</td>
<td align="center">2.0</td>
<td align="center">3.1</td>
<td align="center">0.2</td>
<td align="center">0.5</td>
</tr>
</tbody></table>
<ul>
<li><strong>input_size</strong>代表训练网络时输入网络的图像大小</li>
<li><strong>width_coefficient</strong>代表<strong>channel</strong>维度上的倍率因子，比如在 <strong>EfficientNetB0</strong>中<strong>Stage1</strong>的3x3卷积层所使用的卷积核个数是32，那么在B6中就是32 × 1.8 &#x3D; 57.6 32 \times 1.8&#x3D;57.632×1.8&#x3D;57.6接着取整到离它最近的8的整数倍即56，其它<strong>Stage</strong>同理。</li>
<li><strong>depth_coefficient</strong>代表depth维度上的倍率因子（仅针对<strong>Stage2</strong>到<strong>Stage8</strong>），比如在<strong>EfficientNetB0</strong>中<strong>Stage7</strong>的 $\widehat L_i&#x3D;4$，那么在B6中就是4 × 2.6 &#x3D; 10.4 4 \times 2.6&#x3D;10.44×2.6&#x3D;10.4接着向上取整即11。</li>
<li><strong>drop_connect_rate</strong>是在<strong>MBConv</strong>结构中<strong>dropout</strong>层使用的<strong>drop_rate</strong>，在官方keras模块的实现中<strong>MBConv</strong>结构的<strong>drop_rate</strong>是从0递增到<strong>drop_connect_rate</strong>的（具体实现可以看下官方源码，注意，在源码实现中只有使用<strong>shortcut</strong>的时候才有<strong>Dropout</strong>层）。还需要注意的是，这里的<strong>Dropout</strong>层是<strong>Stochastic Depth</strong>，即会随机丢掉整个<strong>block</strong>的主分支（只剩捷径分支，相当于直接跳过了这个<strong>block</strong>）也可以理解为减少了网络的深度。</li>
<li><strong>dropout_rate</strong>是最后一个全连接层前的<strong>dropout</strong>层（在<strong>stage9</strong>的Pooling与FC之间）的<strong>dropout_rate</strong>。</li>
</ul>
<p>最后是原论文中关于EfficientNet与当时主流网络的性能参数对比:</p>
<p><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220403155412.png"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">方方土同学</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://pengjoy1106.top/2022/04/04/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB3/">http://pengjoy1106.top/2022/04/04/文献阅读3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://pengjoy1106.top" target="_blank">方方土'Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><a class="post-meta__tags" href="/tags/EfficientNet/">EfficientNet</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220410110849.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/04/06/Yolov5%E6%94%AF%E7%BA%BF%E5%AD%A6%E4%B9%A0%E4%B9%8BmAP%E5%80%BC/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220410110942.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">mAP值解析</div></div></a></div><div class="next-post pull-right"><a href="/2022/04/03/%E6%96%87%E4%BB%B6%E5%A4%B9%E6%8C%87%E5%AE%9A%E8%BD%AF%E4%BB%B6%E6%89%93%E5%BC%80/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1641696252196.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">文件夹指定软件打开</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/03/15/Yolov5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/" title="YOLOv5学习笔记1"><img class="cover" src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1641696252204.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-15</div><div class="title">YOLOv5学习笔记1</div></div></a></div><div><a href="/2022/03/17/Yolov5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/" title="YOLOv5学习笔记2"><img class="cover" src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1641696252102.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-17</div><div class="title">YOLOv5学习笔记2</div></div></a></div><div><a href="/2022/03/18/Yolov5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03/" title="Yolov5学习笔记3——源码剖析——Backbone部分1"><img class="cover" src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1641696252121.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-18</div><div class="title">Yolov5学习笔记3——源码剖析——Backbone部分1</div></div></a></div><div><a href="/2022/03/20/Yolov5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05/" title="Yolov5学习笔记5——v5.0源码剖析——Backbone部分2"><img class="cover" src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1641696252121.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-20</div><div class="title">Yolov5学习笔记5——v5.0源码剖析——Backbone部分2</div></div></a></div><div><a href="/2022/03/19/Yolov5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04/" title="Yolov5学习笔记4——源码剖析——Head部分"><img class="cover" src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220410110849.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-19</div><div class="title">Yolov5学习笔记4——源码剖析——Head部分</div></div></a></div><div><a href="/2022/03/21/Yolov5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06/" title="Yolov5学习笔记6——v6.0源码剖析——Backbone部分3"><img class="cover" src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1641696252003.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-21</div><div class="title">Yolov5学习笔记6——v6.0源码剖析——Backbone部分3</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="lv-container" data-id="city" data-uid="MTAyMC81NTgxMC8zMjI3NQ"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/kakarotto (8).jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">方方土同学</div><div class="author-info__description">你知道自己到底想要什么的。对吧？</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/PengJoy1106"><i class="fab fa-github"></i><span>关注</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/PengJoy1106" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:110644319@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">仅以此博客记录我的读研生活。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB3-EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks"><span class="toc-number">1.</span> <span class="toc-text">文献阅读3|EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Intrudoction"><span class="toc-number">1.1.</span> <span class="toc-text">Intrudoction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#compound-scaling-method"><span class="toc-number">1.2.</span> <span class="toc-text">compound scaling method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E6%80%9D%E6%83%B3"><span class="toc-number">1.2.1.</span> <span class="toc-text">论文思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%8D%E5%90%88%E7%BC%A9%E6%94%BECompound-Scaling"><span class="toc-number">1.2.2.</span> <span class="toc-text">复合缩放Compound Scaling</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E8%AF%A6%E7%BB%86%E7%BB%93%E6%9E%84"><span class="toc-number">1.3.</span> <span class="toc-text">网络详细结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MBConv"><span class="toc-number">1.3.1.</span> <span class="toc-text">MBConv</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#EfficientNet-B0-B7-%E5%8F%82%E6%95%B0"><span class="toc-number">1.3.2.</span> <span class="toc-text">EfficientNet(B0-B7)参数</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/04/25/Deep_Learning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/" title="Deep_Learning学习笔记2|优化模型"><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1641696252243.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Deep_Learning学习笔记2|优化模型"/></a><div class="content"><a class="title" href="/2022/04/25/Deep_Learning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/" title="Deep_Learning学习笔记2|优化模型">Deep_Learning学习笔记2|优化模型</a><time datetime="2022-04-25T15:00:00.000Z" title="发表于 2022-04-25 23:00:00">2022-04-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/09/%E7%A2%8E%E7%A2%8E%E5%BF%B51/" title="碎碎念1"><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220410110942.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="碎碎念1"/></a><div class="content"><a class="title" href="/2022/04/09/%E7%A2%8E%E7%A2%8E%E5%BF%B51/" title="碎碎念1">碎碎念1</a><time datetime="2022-04-09T14:00:00.000Z" title="发表于 2022-04-09 22:00:00">2022-04-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/06/Yolov5%E6%94%AF%E7%BA%BF%E5%AD%A6%E4%B9%A0%E4%B9%8BmAP%E5%80%BC/" title="mAP值解析"><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220410110942.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="mAP值解析"/></a><div class="content"><a class="title" href="/2022/04/06/Yolov5%E6%94%AF%E7%BA%BF%E5%AD%A6%E4%B9%A0%E4%B9%8BmAP%E5%80%BC/" title="mAP值解析">mAP值解析</a><time datetime="2022-04-06T14:00:00.000Z" title="发表于 2022-04-06 22:00:00">2022-04-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/04/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB3/" title="文献阅读3|EfficientNet"><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220410110849.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读3|EfficientNet"/></a><div class="content"><a class="title" href="/2022/04/04/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB3/" title="文献阅读3|EfficientNet">文献阅读3|EfficientNet</a><time datetime="2022-04-04T14:00:00.000Z" title="发表于 2022-04-04 22:00:00">2022-04-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/03/%E6%96%87%E4%BB%B6%E5%A4%B9%E6%8C%87%E5%AE%9A%E8%BD%AF%E4%BB%B6%E6%89%93%E5%BC%80/" title="文件夹指定软件打开"><img src="https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1641696252196.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文件夹指定软件打开"/></a><div class="content"><a class="title" href="/2022/04/03/%E6%96%87%E4%BB%B6%E5%A4%B9%E6%8C%87%E5%AE%9A%E8%BD%AF%E4%BB%B6%E6%89%93%E5%BC%80/" title="文件夹指定软件打开">文件夹指定软件打开</a><time datetime="2022-04-03T15:00:00.000Z" title="发表于 2022-04-03 23:00:00">2022-04-03</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;undefined - 2022  <i id="heartbeat" class="fa fas fa-heartbeat"></i> 方方土同学</div><div class="footer_custom_text">💛所以你好，再见。💙</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function loadLivere () {
  if (typeof LivereTower === 'object') {
    window.LivereTower.init()
  }
  else {
    (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
    })(document, 'script');
  }
}

if ('Livere' === 'Livere' || !true) {
  if (true) btf.loadComment(document.getElementById('lv-container'), loadLivere)
  else loadLivere()
}
else {
  function loadOtherComment () {
    loadLivere()
  }
}</script></div><div class="aplayer no-destroy" data-id="2914746287" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="false" data-lrctype=1 list-folded="true"> </div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>